{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Команда \"Друзья, цели, победы\"\n",
    "## Спеллчекер с проверкой по частотным биграммам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cоздаем словарь из списка биграмм OpenCorpora, где ключ -- биграмма, а значение -- ipm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('bigrams.cyrB.txt', 'r', encoding = 'utf-8')\n",
    "bigrams1 = file.readlines()\n",
    "our_bigrams = []\n",
    "for bigram in bigrams1:\n",
    "    our_bigrams.append(bigram.split('\\t'))\n",
    "for bigram in our_bigrams:\n",
    "    bigram[2] = int(bigram[1].strip('\\n'))\n",
    "    del bigram[1]\n",
    "bigrams_ipm = {b[0]:b[1] for b in our_bigrams}\n",
    "bigrams_ipm = {tuple(k.split(' ')):int(v) for k,v in bigrams_ipm.items()}\n",
    "\n",
    "file = open('unigrams.txt', 'r', encoding = 'utf-8')\n",
    "words1 = file.readlines()\n",
    "our_words = []\n",
    "for word in words1:\n",
    "    our_words.append(word.split('\\t'))\n",
    "for word in our_words:\n",
    "    del word[2]\n",
    "    word[0] = word[0].lower()\n",
    "words_ipm = {w[0]:int(w[1]) for w in our_words}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Питера Норвига"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS1 = dict(Counter(words(open('dict.opcorpora1.txt').read())))\n",
    "\n",
    "WORDS = {**WORDS1, **words_ipm}\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    if word in WORDS:\n",
    "        return WORDS[word] / N\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка по частотным биграммам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(string):\n",
    "    string = string.lower()\n",
    "    string = string.strip()\n",
    "    #string = ' '.join(re.findall('[а-яё|-]+', string))\n",
    "    string = ' '.join(re.findall('[\\w-]+', string))\n",
    "    return string\n",
    "\n",
    "def mistake_check(string):\n",
    "    #функция находит ошибочные слова в предложении\n",
    "    mistakes = []\n",
    "    string = string.split()\n",
    "    for word in string:\n",
    "        if word not in WORDS:\n",
    "            mistakes.append(word)\n",
    "    return list(set(mistakes))\n",
    "\n",
    "def generate_bigrams(string):\n",
    "    #функция генерирует биграммы \n",
    "    return [gram for gram in bigrams(string.split())]\n",
    "\n",
    "def enumerate_bigrams(string):\n",
    "    return enumerate(generate_bigrams(string))\n",
    "\n",
    "def known_bigrams(grams):\n",
    "    #функция проверяет, какие биграммы из полученных находятся в словаре биграмм, \n",
    "    # понадобится позже, чтобы проверять биграммы-кандидаты\n",
    "    return list(set(gram for gram in grams if gram in bigrams_ipm.keys()))\n",
    "    #return set(w for w in words if w in WORDS)\n",
    "\n",
    "def wrong_bigrams(string):\n",
    "    #выдает сет биграмм с ошибочными словами\n",
    "    wrong_bigrams = []\n",
    "    mistakes = mistake_check(string)\n",
    "    bbigrams = generate_bigrams(string)\n",
    "    for gram in bbigrams:\n",
    "        for word in gram:\n",
    "            if word in mistakes:\n",
    "                wrong_bigrams.append(gram)\n",
    "    return list(set(wrong_bigrams))\n",
    "\n",
    "def possible_bigrams(string):\n",
    "    a = wrong_bigrams(string)\n",
    "    possible_bigrams = []\n",
    "    for w1,w2 in a:\n",
    "        possible_bigrams_one = []\n",
    "        if w1 in mistake_check(string) and w2 not in mistake_check(string):\n",
    "            for word in list(candidates(w1)):\n",
    "                empty_bigram = (word, w2)\n",
    "                possible_bigrams_one.append(empty_bigram)\n",
    "                empty_bigram = ((), ())\n",
    "        elif w2 in mistake_check(string) and w1 not in mistake_check(string):\n",
    "            for word in list(candidates(w2)):\n",
    "                empty_bigram = (w1, word)\n",
    "                possible_bigrams_one.append(empty_bigram)\n",
    "                empty_bigram = ((), ())\n",
    "        elif w1 and w2 in mistake_check(string):\n",
    "            for word in list(candidates(w1)):\n",
    "                for word2 in list(candidates(w2)):\n",
    "                    empty_bigram = (word, word2)\n",
    "                    possible_bigrams_one.append(empty_bigram)\n",
    "                    empty_bigram = ((), ())\n",
    "        possible_bigrams.append(possible_bigrams_one)\n",
    "    return dict(zip(a,possible_bigrams))\n",
    "\n",
    "def likely_bigrams(string):\n",
    "    d = possible_bigrams(string)\n",
    "    for key in d:\n",
    "        if known_bigrams(d[key]):\n",
    "            d[key] = known_bigrams(d[key])\n",
    "        else:\n",
    "            pass\n",
    "    return d\n",
    "\n",
    "\n",
    "def choose_bigram(string):\n",
    "    d = likely_bigrams(string)\n",
    "    e = {}\n",
    "    for key in d.keys():\n",
    "        if d[key] != []:\n",
    "            for item in d[key]:\n",
    "                e[item] = bigrams_ipm.get(item)\n",
    "                if e[item] == None:\n",
    "                    e[item] = 0\n",
    "            d[key] = max(e.items(), key=operator.itemgetter(1))[0]\n",
    "            e = {}\n",
    "        else: d[key] = 'NO'\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replace_bigram(string):\n",
    "    d = choose_bigram(string)\n",
    "    a = generate_bigrams(string)\n",
    "    for i, gram in enumerate(a):\n",
    "        if gram in d.keys():\n",
    "            if d[gram] != 'NO':\n",
    "                a[i] = d[gram]\n",
    "    return (a)\n",
    "\n",
    "def restore_sentence(bigrams):\n",
    "    sentence = []\n",
    "    correct_sentence = []\n",
    "    for w1,w2 in bigrams:\n",
    "        sentence.append(w1)\n",
    "        sentence.append(w2)\n",
    "    if sentence[0] not in WORDS:\n",
    "        correct_sentence.append(correction(sentence[0]))\n",
    "    else:\n",
    "        correct_sentence.append(sentence[0])\n",
    "    for i in range(1, len(sentence)-1, 2):\n",
    "        if sentence[i] in WORDS and sentence[i+1] not in WORDS:\n",
    "            correct_sentence.append(sentence[i])\n",
    "        elif sentence[i] not in WORDS and sentence[i+1] in WORDS:\n",
    "            correct_sentence.append(sentence[i+1])\n",
    "        elif sentence[i] == sentence[i+1]:\n",
    "            correct_sentence.append(sentence[i])\n",
    "        elif sentence[i] in WORDS and sentence[i+1] in WORDS:\n",
    "            correct_sentence.append(max(set([sentence[i], sentence[i+1]]), key=P))\n",
    "    if sentence[-1] not in WORDS:\n",
    "        correct_sentence.append(correction(sentence[-1]))\n",
    "    else:\n",
    "        correct_sentence.append(sentence[-1])\n",
    "    correct_sentence = ' '.join(correct_sentence)\n",
    "    return correct_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Финал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sentence(string):\n",
    "    string = preprocessing(string)\n",
    "    string = replace_bigram(string)\n",
    "    string = restore_sentence(string)\n",
    "    string = string.split()\n",
    "    string = ' '.join(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что получилось?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct sentence:\n",
      " притеки неандертальского искусства утверждают что никакого такого искусства нет\n"
     ]
    }
   ],
   "source": [
    "ex = 'Критеки ниандертальсково искуства утверждают, что никакого такого исскусства нет'\n",
    "print('Correct sentence:\\n', correct_sentence(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим evaluation-скриптом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008\n"
     ]
    }
   ],
   "source": [
    "with open('test_new.txt') as f:\n",
    "    lines = f.readlines()\n",
    "clean_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    clean_lines.append(line)\n",
    "to_be_corrected = clean_lines[::3]\n",
    "correct_answers = clean_lines[1::3]\n",
    "print(len(to_be_corrected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ну вот собственно и день весь вышел\\n', 'коридоры казались огромными было довольно пусто мы бродили по лестницам этажам\\n', 'а так все принципе нормально\\n', 'надеюсь порисовать но посмотрю какая бдеть погода\\n', 'хотела посчитать скока я за седня выпила воды\\n', 'приметы прыщи на бороде\\n', 'сейчас она победила на конкурсе русского языка в японии и получила бесплатный билет в россию\\n', 'впрочем официальные портреты не всегда высокого качества увы востребованы и другими менее именитыми особами\\n', 'оказывается можно быть счастливой и несчастной одновременно\\n', 'сегодня яичницей никто не завтракал как впрочем и вчера на ближайшем к нам рынке мы ели фруктовый салат со свежеотжатым соком как в старые добрые времена в бразилии\\n']\n"
     ]
    }
   ],
   "source": [
    "new_sents = []\n",
    "for sent in to_be_corrected:\n",
    "    sent = correct_sentence(sent)\n",
    "    sentence = sent + '\\n'\n",
    "    new_sents.append(sentence)\n",
    "print(new_sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = open('corrected_3.txt', 'w')\n",
    "fl.write(''.join(new_sents)) \n",
    "fl.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = open('to_be_corrected.txt', 'w')\n",
    "fl.write('\\n'.join(to_be_corrected)) \n",
    "fl.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = open('correct_answers.txt', 'w')\n",
    "fl.write('\\n'.join(correct_answers)) \n",
    "fl.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = list(range(len(to_be_corrected)))\n",
    "indexes = ''\n",
    "for i in index_list:\n",
    "    indexes += str(i)\n",
    "    indexes += '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_1 = open('index_file.txt', 'w')\n",
    "fl_1.write(indexes)\n",
    "fl_1.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты\n",
    "Precision=40.78\n",
    "Recall=38.02\n",
    "FMeasure=39.36\n",
    "Accuracy=47.22 <br>\n",
    "354 868 931"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
